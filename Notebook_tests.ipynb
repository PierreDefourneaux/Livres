{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c80882f-176e-4e78-8885-b1e55db79881",
   "metadata": {},
   "source": [
    "<h1 style=\"color:#c6b080; font-size:30px; text-align:center; background-color:#cefffc; padding:20px; border-radius:10px;\">Projet bloc 1 : moteur de recommendations de livres</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d1e92c-0225-4cf9-a4e6-6f47884941c7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "<h1>Script de l'extraction automatisée des données</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19702337-f581-430e-9e3b-010c299489e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import time\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "import sys\n",
    "import xml.etree.ElementTree as ET\n",
    "from deep_translator import GoogleTranslator\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import base64\n",
    "from tqdm import tqdm\n",
    "\n",
    "start_time = time.time()  \n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "token_librarything_API = os.getenv('TOKEN_LIBRARYTHING_API')\n",
    "\n",
    "print(\"\\nDébut de l'exécution du script :\",datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "print(\"Environnement virtuel requis : env_wsl_bloc_1.\")\n",
    "print(\"Environnement virtuel et Python utilisés =\", sys.executable)\n",
    "\n",
    "############################################################## Définition des fonctions d'extraction de données\n",
    "def is_french_isbn(isbn):\n",
    "    \"\"\"Détermine si l'ISBN est français ou pas selon selon la détection des préfixes :\n",
    "    \"9782\" ou \"97910\" pour les ISBN13\n",
    "    \"2\" pour les ISBN10.\"\"\"\n",
    "    isbn = isbn.replace(\"-\", \"\").replace(\" \", \"\")\n",
    "    if isbn.startswith(\"9782\") or isbn.startswith(\"97910\") or isbn.startswith(\"2\"):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def find_french_isbn_in_list(isbns):\n",
    "    \"\"\"Parcourt une liste d'ISBN jusqu'à en trouver un qui soit français, puis le retourne.\n",
    "    Détermine si l'ISBN est français ou pas selon selon la détection des préfixes :\n",
    "    \"9782\" ou \"97910\" pour les ISBN13\n",
    "    \"2\" pour les ISBN10.  \"\"\"\n",
    "    count = 0\n",
    "    while count < len(isbns):\n",
    "        isbn = isbns[count]\n",
    "        isbn = isbn.replace(\"-\", \"\").replace(\" \", \"\")\n",
    "        if isbn.startswith(\"9782\") or isbn.startswith(\"97910\") or isbn.startswith(\"2\"):\n",
    "            return isbn\n",
    "        count += 1\n",
    "    return None\n",
    "\n",
    "def get_isbn_with_librarything(token_librarything_API,title):\n",
    "    \"\"\"Retourne une liste de tous les ISBN connus par l'API librarything pour ce livre à partir du titre donné.\n",
    "    Ecrivez le titre naturellement en conservant les espaces.\"\"\"\n",
    "    title = title.replace(' ', '+')\n",
    "    url = f\"https://www.librarything.com/api/{token_librarything_API}/thingTitle/{title}\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        root = ET.fromstring(response.content.decode('utf-8'))\n",
    "        liste = []\n",
    "        for elem in root.iter(\"isbn\"):\n",
    "            liste.append(elem.text)\n",
    "        return liste\n",
    "    else :\n",
    "        return None\n",
    "\n",
    "def get_book_from_isbn(isbn):\n",
    "    \"\"\"lance un requests.get(url) vers une page de librarything depuis l'ISBN\"\"\"\n",
    "    url = f\"https://www.librarything.com/isbn/{isbn}\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        print(\"requete reussie, pret a scraper\")\n",
    "    else:\n",
    "        print(f\"Erreur lors de la requête https://www.librarything.com/isbn/ : {response.status_code}\")\n",
    "\n",
    "def translate_title_to_french(titre):\n",
    "    traduction = GoogleTranslator(source='auto', target='fr').translate(titre)\n",
    "    return traduction\n",
    "\n",
    "def google_books_api(title, maxresults = 15):\n",
    "    \"\"\"interroge google_books_api avec le titre d'un livre. \n",
    "    Retourne un tuple data, title, page_count, isbn_fr, author, theme, response.status_code.\n",
    "    Argument 1 = titre du livre (on peut laisser les espaces)\n",
    "    Argument 2 = nombre de résultats maximal pour la requête\"\"\"\n",
    "    url = \"https://www.googleapis.com/books/v1/volumes\"\n",
    "    params = {\n",
    "        \"q\": f\"intitle:{title}\",\n",
    "        \"maxResults\": maxresults\n",
    "    }\n",
    "    response = requests.get(url, params=params)\n",
    "    data = response.json()\n",
    "    if \"items\" in data:\n",
    "        ######################################################### Extraire le titre\n",
    "        title = data[\"items\"][0][\"volumeInfo\"][\"title\"]\n",
    "        if not title :\n",
    "            title = None\n",
    "        ######################################################### Extraire le nombre de pages\n",
    "        page_count = 0\n",
    "        if data.get(\"items\"):\n",
    "            for livre in data[\"items\"]:\n",
    "                if livre.get(\"volumeInfo\"):\n",
    "                    volumeInfo = livre.get(\"volumeInfo\")\n",
    "                    if volumeInfo.get(\"pageCount\"):\n",
    "                        if volumeInfo.get(\"pageCount\") > 5:\n",
    "                            page_count = volumeInfo.get(\"pageCount\")\n",
    "        if not page_count:\n",
    "            page_count = None\n",
    "        ######################################################### Extraire un ISBN français\n",
    "        isbn_fr = \"\"\n",
    "        if data.get(\"items\"):\n",
    "            for livre in data[\"items\"]:\n",
    "                if livre.get(\"volumeInfo\"):\n",
    "                    volumeInfo = livre.get(\"volumeInfo\")\n",
    "                    if volumeInfo.get(\"industryIdentifiers\"):\n",
    "                        industryIdentifiers = volumeInfo.get(\"industryIdentifiers\")\n",
    "                        for i in industryIdentifiers :\n",
    "                            if i.get(\"identifier\"):\n",
    "                                code = i.get(\"identifier\")\n",
    "                                if len(code) == 10 and code.startswith(\"2\"):\n",
    "                                    isbn_fr = code \n",
    "                                if code.startswith(\"9782\") or code.startswith(\"97910\"):\n",
    "                                    isbn_fr = code\n",
    "        if not isbn_fr:\n",
    "            isbn_fr = None   \n",
    "        ######################################################### Extraire l'auteur\n",
    "        author = \"\"\n",
    "        if data.get(\"items\"):\n",
    "            for livre in data[\"items\"]:\n",
    "                if livre.get(\"volumeInfo\"):\n",
    "                    volumeInfo = livre.get(\"volumeInfo\")\n",
    "                    if volumeInfo.get(\"authors\"):\n",
    "                        author = volumeInfo.get(\"authors\")\n",
    "        if not author:\n",
    "            author = None\n",
    "\n",
    "        ######################################################### Extraire les thèmes\n",
    "        theme = \"\"\n",
    "        if data.get(\"items\"):\n",
    "            for livre in data[\"items\"]:\n",
    "                if livre.get(\"volumeInfo\"):\n",
    "                    volumeInfo = livre.get(\"volumeInfo\")\n",
    "                    if volumeInfo.get(\"categories\"):\n",
    "                        theme = str(volumeInfo.get(\"categories\"))\n",
    "        if not theme:\n",
    "            theme = None\n",
    "        \n",
    "        return data, title, page_count, isbn_fr, author, theme, response.status_code\n",
    "    else : \n",
    "        return None, None, None, None, None, None, response.status_code\n",
    "\n",
    "########################################################################################## fin de la définition des fonctions\n",
    "########################################################################################## début du scraping\n",
    "# print(\"\\nDébut du webscraping de Openlibrary.org\")\n",
    "\n",
    "titres = []\n",
    "notes = []\n",
    "auteurs = []\n",
    "dates_de_publication = []\n",
    "editeurs = []\n",
    "nombres_de_pages = []\n",
    "themes_liste = []\n",
    "isbn10_français = []\n",
    "isbn13_français = []\n",
    "isbn_defaut = []\n",
    "ISBN_verifie = []\n",
    "images_openlibrary = []\n",
    "\n",
    "for i in tqdm(range(1,2), desc=\"Scraping des 10 pages \\\"les livres en vogue du moment\\\" de Openlibrary.org\"):\n",
    "    url_scrap = f\"https://openlibrary.org/trending/now?page={i}\"\n",
    "    response = requests.get(url_scrap)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        liens = soup.find_all(\"a\", class_=\"results\")\n",
    "        # print(\"\\n\\n\\nextraction des liens et redirection vers les livres de la page\",i)\n",
    "        for title in liens:\n",
    "            url2 = f\"\"\"https://openlibrary.org/{title.get(\"href\")}\"\"\"\n",
    "            response2 = requests.get(url2)\n",
    "            if response2.status_code == 200:\n",
    "                soup2 = BeautifulSoup(response2.text, \"html.parser\")\n",
    "                \n",
    "                try:\n",
    "                    titre = soup2.find(\"h1\", itemprop=\"name\").text\n",
    "                    titres.append(titre)\n",
    "                    # print(\"\\nExtraction des informations du livre\",titre)\n",
    "                except Exception as e:\n",
    "                    # print(\"Echec de récupération du titre dans\\n\\t\\t\\t\",url2,\"\\n\\t\\t\\t\",e,\"\\n\")\n",
    "                    titres.append(\"indisponible\")\n",
    "                    titre = \"indisponible\"\n",
    "                \n",
    "                #exctraction des ISBN 10\n",
    "                try:\n",
    "                    isbn = soup2.find_all(\"dd\", attrs={\"itemprop\": \"isbn\"})\n",
    "                    french_isbn = \"\"\n",
    "                    for number in isbn:\n",
    "                        s = number.text\n",
    "                        s = re.sub(r\"[^A-Z0-9]\", \"\", s)\n",
    "                        if len(s) == 10: \n",
    "                            if is_french_isbn(s):\n",
    "                                french_isbn = s\n",
    "                            else:\n",
    "                                def_isbn = s\n",
    "                    if french_isbn:\n",
    "                        value10 = french_isbn\n",
    "                    else :\n",
    "                        value10 = \"indisponible\"\n",
    "                except Exception as e:\n",
    "                    value10 = \"indisponible\"\n",
    "                    \n",
    "                isbn10_français.append(value10)\n",
    "\n",
    "               #exctraction des ISBN 13\n",
    "                try:\n",
    "                    isbn = soup2.find_all(\"dd\", attrs={\"itemprop\": \"isbn\"})\n",
    "                    french_isbn = \"\"\n",
    "                    def_isbn =\"\"\n",
    "                    for number in isbn:\n",
    "                        s = number.text\n",
    "                        s = re.sub(r\"[^A-Z0-9]\", \"\", s)\n",
    "                        if len(s) == 13: \n",
    "                            if is_french_isbn(s):\n",
    "                                french_isbn = s\n",
    "                            else:\n",
    "                                def_isbn = s\n",
    "                    if french_isbn:\n",
    "                        value13 = french_isbn\n",
    "                    else :\n",
    "                        value13 = \"indisponible\"\n",
    "                except Exception as e:\n",
    "                    value13 = \"indisponible\"\n",
    "                    \n",
    "                isbn13_français.append(value13)\n",
    "\n",
    "                if def_isbn:\n",
    "                    isbn_defaut.append(def_isbn)\n",
    "                else:\n",
    "                    def_isbn = \"indisponible\"\n",
    "                    isbn_defaut.append(def_isbn)\n",
    "                    \n",
    "                if value10 == \"indisponible\" and value13 == \"indisponible\":\n",
    "                    # print(\"\"\"\\tAucun ISBN français trouvé pendant le scraping d'Openlibrary.org.\\n\\t\\tRequête à l'API Librarything en cours pour tenter de récupérer l'ISBN français.\"\"\")\n",
    "                    res = get_isbn_with_librarything(token_librarything_API,titre)\n",
    "                    if res:\n",
    "                        # print(\"\\t\\tLibrarything propose\",len(res),\"ISBN...\")\n",
    "                        value = find_french_isbn_in_list(res)\n",
    "                        if value:\n",
    "                            if len(value) == 10:\n",
    "                                value10 = value\n",
    "                                isbn10_français.pop()\n",
    "                                isbn10_français.append(value)\n",
    "                                # print(\"\\t\\t...un ISBN 10 français a pu être extrait :\",value)\n",
    "                            if len(value) == 13:\n",
    "                                value13 = value\n",
    "                                isbn13_français.pop()\n",
    "                                isbn13_français.append(value)\n",
    "                                # print(\"\\t\\t... un ISBN 13 français a pu être extrait :\",value)\n",
    "                        if not value :\n",
    "                            # print(\"\\t\\t... mais aucun n'est français.\\n\\t\\tNouvelle tentative en cours vers l'API google books.\")\n",
    "                            a = google_books_api(titre)\n",
    "                            if a[3]:\n",
    "                                # print(\"\\t\\tRésultat avec google books =\",a[3])\n",
    "                                if a[3] == 10:\n",
    "                                    value10 = a[3]\n",
    "                                    isbn10_français.pop()\n",
    "                                    isbn10_français.append(value10)\n",
    "                                else:\n",
    "                                    value13 = a[3]\n",
    "                                    isbn13_français.pop()\n",
    "                                    isbn13_français.append(value13)\n",
    "                            else :\n",
    "                                # print(\"\\t\\tGoogle books n'a pas trouvé d'ISBN français pour\", titre)\n",
    "                                # print(\"\\t\\tNouvelle tentative avec cette traduction du titre :\", translate_title_to_french(titre))\n",
    "                                res_translated = google_books_api(translate_title_to_french(titre),3)[3]\n",
    "                                if res_translated:\n",
    "                                    # print(\"\\t\\tAvec ce nouveau titre, Google books API a trouvé cet ISBN français:\",res_translated)\n",
    "                                    if res_translated == 10:\n",
    "                                        value10 = res_translated\n",
    "                                        isbn10_français.pop()\n",
    "                                        isbn10_français.append(value10)\n",
    "                                    else:\n",
    "                                        value13 = res_translated\n",
    "                                        isbn13_français.pop()\n",
    "                                        isbn13_français.append(value13)\n",
    "                                # else :\n",
    "                                    # print(\"\\t\\tLa traduction du titre n'a pas donné de meilleurs résultats.\")\n",
    "                    else:\n",
    "                        # print(\"\\t\\tAucun ISBN trouvé avec l'API de librarything.\\n\\t\\tNouvelle tentative en cours vers l'API google books.\")\n",
    "                        a = google_books_api(titre)\n",
    "                        if a[3]:\n",
    "                            # print(\"\\t\\tRésultat avec google books =\",a[3])\n",
    "                            if a[3] == 10:\n",
    "                                value10 = a[3]\n",
    "                                isbn10_français.pop()\n",
    "                                isbn10_français.append(value10)\n",
    "                            else:\n",
    "                                value13 = a[3]\n",
    "                                isbn13_français.pop()\n",
    "                                isbn13_français.append(value13)\n",
    "                        else :\n",
    "                            # print(\"\\t\\tGoogle books n'a pas trouvé d'ISBN français pour\", titre)\n",
    "                            # print(\"\\t\\tNouvelle tentative avec cette traduction du titre :\", translate_title_to_french(titre))\n",
    "                            a = google_books_api(translate_title_to_french(titre),3)[3]\n",
    "                            if a:\n",
    "                                # print(\"\\t\\tAvec ce nouveau titre, Google books API a trouvé cet ISBN français:\",a)\n",
    "                                if a == 10 :\n",
    "                                    isbn10_français.pop(0)\n",
    "                                    isbn10_français.append(a)\n",
    "                                    value10 = a\n",
    "                                else:\n",
    "                                    isbn13_français.pop()\n",
    "                                    isbn13_français.append(a)\n",
    "                                    value13 = a\n",
    "                            # else :\n",
    "                                # print(\"\\t\\tLa traduction du titre n'a pas donné de meilleurs résultats.\")\n",
    "                                \n",
    "                # if value10 == \"indisponible\" and value13 == \"indisponible\" and def_isbn == \"indisponible\":\n",
    "                    # print(\"\\t\\tAucun isbn extrait.\")\n",
    "\n",
    "                ISBN_verifie.append(\"ISBN pas encore vérifié\")\n",
    "                #exctraction des notes    \n",
    "                try:\n",
    "                    note = soup2.find(\"span\", itemprop=\"ratingValue\").text\n",
    "                    # note = re.sub(r\"\\(.*?\\)\", \"\", note)\n",
    "                    notes.append(note)\n",
    "                except Exception as e:\n",
    "                    # print(\"\\tEchec de récupération de la note.\")\n",
    "                    notes.append(\"indisponible\")\n",
    "\n",
    "                #extraction des auteurs\n",
    "                try:\n",
    "                    auteur = soup2.find(\"a\", itemprop=\"author\").text\n",
    "                    if auteur :\n",
    "                        auteurs.append(auteur)\n",
    "                    else :\n",
    "                        # print(\"\\tPas d'auteur extrait depuis Openlibrary.\")\n",
    "                        a = str(google_books_api(titre)[4])\n",
    "                        if a:\n",
    "                            auteurs.append(a)\n",
    "                            # print(\"\\t\\tAuteur récuépré via Google_books_API :\",a)\n",
    "                        else:\n",
    "                            # print(\"\\t\\tEchec de récupération de l'auteur, même après une deuxième tentative sur google_books_API.\")\n",
    "                            auteurs.append(\"indisponible\")\n",
    "                except Exception as e:\n",
    "                    # print(\"\\tEchec de récupération de l'auteur.\")\n",
    "                    auteurs.append(\"indisponible\")\n",
    "\n",
    "                #extraction des dates de publication\n",
    "                try:\n",
    "                    date_de_publication = soup2.find(\"span\", itemprop=\"datePublished\").text\n",
    "                    dates_de_publication.append(date_de_publication)\n",
    "                except Exception as e:\n",
    "                    # print(\"\\tEchec de récupération de la date de publication.\")\n",
    "                    dates_de_publication.append(\"indisponible\")\n",
    "\n",
    "                #extraction des éditeurs\n",
    "                try:\n",
    "                    editeur = soup2.find(\"a\", itemprop=\"publisher\").text\n",
    "                    editeurs.append(editeur)\n",
    "                except Exception as e:\n",
    "                    # print(\"\\tEchec de récupération de l'éditeur.\")\n",
    "                    editeurs.append(\"indisponible\")\n",
    "\n",
    "                #extraction des nombres de pages\n",
    "                try:\n",
    "                    nombre_de_pages = soup2.find(\"span\", itemprop=\"numberOfPages\").text\n",
    "                    if nombre_de_pages:\n",
    "                        if int(nombre_de_pages) > 5: \n",
    "                            nombres_de_pages.append(int(nombre_de_pages))\n",
    "                        else:\n",
    "                            nombres_de_pages.append(\"indisponible\")\n",
    "                    else :\n",
    "                        # print(\"\\tPas de nombre de pages extrait depuis Openlibrary.\")\n",
    "                        a = google_books_api(titre)[2]\n",
    "                        if a and int(a) > 5:\n",
    "                            # print(\"\\t\\tNombre de pages récuépré via Google_books_API :\",a)\n",
    "                            nombres_de_pages.append(int(a))\n",
    "                        else:\n",
    "                            # print(\"\\t\\tEchec de récupération du nombre de pages, même après une deuxième tentative sur google_books_API.\")\n",
    "                            nombres_de_pages.append(\"indisponible\")\n",
    "                except Exception as e:\n",
    "                    # print(\"\\tEchec de récupération du nombre de pages.\")\n",
    "                    a = google_books_api(titre)[2]\n",
    "                    if a and int(a) > 5:\n",
    "                        # print(\"\\t\\tNombre de pages récupéré via Google_books_API :\",a)\n",
    "                        nombres_de_pages.append(int(a))\n",
    "                    else:\n",
    "                        # print(\"\\t\\tEchec de récupération du nombre de pages, même après une deuxième tentative sur google_books_API.\")\n",
    "                        nombres_de_pages.append(\"indisponible\")\n",
    "        \n",
    "                #extraction des thèmes\n",
    "                try:\n",
    "                    themes_tags = soup2.find_all(\"a\", attrs={\"data-ol-link-track\": \"BookOverview|SubjectClick\"})\n",
    "                    themes_sous_liste = []\n",
    "                    for tag in themes_tags :\n",
    "                        themes_sous_liste.append(tag.text)\n",
    "                    if themes_sous_liste:\n",
    "                        themes_liste.append(themes_sous_liste)\n",
    "                    else:\n",
    "                        # print(\"\\tEchec de récupération des thèmes\")\n",
    "                        theme = google_books_api(titre)[5]\n",
    "                        if theme :\n",
    "                            themes_liste.append(theme)\n",
    "                            # print(\"\\t\\tThèmes récupérés via Google_books_API\")\n",
    "                        else:\n",
    "                            themes_liste.append(\"indisponible\")\n",
    "                            # print(\"\\t\\tEchec également avec Google_books_API\")\n",
    "                except Exception as e:\n",
    "                    # print(\"\\tEchec de récupération des thèmes\")\n",
    "                    theme = google_books_api(titre)[5]\n",
    "                    if theme :\n",
    "                        themes_liste.append(theme)\n",
    "                        # print(\"\\t\\tThèmes récupérés via Google_books_API\")\n",
    "                    else:\n",
    "                        themes_liste.append(\"indisponible\")\n",
    "                        # print(\"\\t\\tEchec également avec Google_books_API\")\n",
    "\n",
    "                #récupération des images\n",
    "                try:\n",
    "                    boite_image = soup2.find(\"div\", class_=\"illustration edition-cover\")\n",
    "                    img_tag = boite_image.find(\"img\")\n",
    "                    img_url = \"https:\" + img_tag[\"src\"]\n",
    "                    response_img = requests.get(img_url)\n",
    "                    if response_img.status_code == 200 :\n",
    "                        image_64 = base64.b64encode(response_img.content).decode('utf-8')\n",
    "                        if image_64:\n",
    "                            images_openlibrary.append(image_64)          \n",
    "                        else:\n",
    "                            images_openlibrary.append(\"indisponible\")\n",
    "                    else :\n",
    "                        images_openlibrary.append(\"indisponible\")\n",
    "                except Exception as e:\n",
    "                    # print(\"\\tEchec de récupération de l'image dans\\n\\t\\t\",url2,\"\\n\\t\\t\",e,\"\\n\")\n",
    "                    images_openlibrary.append(\"indisponible\")\n",
    "            else:\n",
    "                print(\"\\t!!redirection vers la description du livre ratée, erreur =\",response2.status_code)\n",
    "    else:\n",
    "        print(\"\\t!!\", datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"), \"echec de la requete \",url_scrap, \" status_code erreur =\",response.status_code)\n",
    "\n",
    "\n",
    "\n",
    "themes = []\n",
    "for i in themes_liste:\n",
    "    themes.append(str(i))    \n",
    "\n",
    "data_webscrap_a_nettoyer = {\n",
    "    \"livre_nom\" : titres,\n",
    "    \"livre_note\" : notes,\n",
    "    \"livre_nombre_de_pages\" : nombres_de_pages,\n",
    "    \"livre_date\" : dates_de_publication,\n",
    "    \"ISBN_10\" : isbn10_français,\n",
    "    \"ISBN_13\" : isbn13_français,\n",
    "    \"ISBN_defaut\" : isbn_defaut,\n",
    "    \"ISBN_verifie\" : ISBN_verifie,\n",
    "    \"thematique_nom\" : themes,\n",
    "    \"editeur_nom\" : editeurs,\n",
    "    \"auteur_nom\" : auteurs,\n",
    "    \"images_openlibrary\" : images_openlibrary\n",
    "    }\n",
    "\n",
    "print(len(titres),\"titres\")\n",
    "print(len(notes),\"notes\")\n",
    "print(len(nombres_de_pages),\"nombres de pages\")\n",
    "print(len(dates_de_publication),\"dates\")\n",
    "print(len(isbn10_français),\"isbn 10\")\n",
    "print(len(isbn13_français),\"isbn 13\")\n",
    "print(len(isbn_defaut),\"isbn_defaut\")\n",
    "print(len(themes),\"thèmes\")\n",
    "print(len(editeurs),\"editeurs\")\n",
    "print(len(auteurs),\"auteurs\")\n",
    "print(len(images_openlibrary),\"images_openlibrary\")\n",
    "\n",
    "df_data_webscrap_a_nettoyer = pd.DataFrame(data_webscrap_a_nettoyer)\n",
    "\n",
    "missing_livres = df_data_webscrap_a_nettoyer['livre_nom'].value_counts().get('indisponible', 0)\n",
    "missing_notes = df_data_webscrap_a_nettoyer['livre_note'].value_counts().get('indisponible', 0)\n",
    "missing_auteurs = df_data_webscrap_a_nettoyer['auteur_nom'].value_counts().get('indisponible', 0)\n",
    "missing_dates = df_data_webscrap_a_nettoyer['livre_date'].value_counts().get('indisponible', 0)\n",
    "missing_editeurs = df_data_webscrap_a_nettoyer['editeur_nom'].value_counts().get('indisponible', 0)\n",
    "missing_pages = df_data_webscrap_a_nettoyer['livre_nombre_de_pages'].value_counts().get('indisponible', 0)\n",
    "missing_themes = df_data_webscrap_a_nettoyer['thematique_nom'].value_counts().get('indisponible', 0)\n",
    "missing_isbn = ((df_data_webscrap_a_nettoyer[\"ISBN_10\"] == \"indisponible\") & (df_data_webscrap_a_nettoyer[\"ISBN_13\"] == \"indisponible\")& (df_data_webscrap_a_nettoyer[\"ISBN_defaut\"] == \"indisponible\")).sum()\n",
    "missing_images = df_data_webscrap_a_nettoyer['images_openlibrary'].value_counts().get('indisponible', 0)\n",
    "total_missing = int(missing_images)+int(missing_livres)+int(missing_notes)+int(missing_auteurs)+int(missing_dates)+int(missing_editeurs)+int(missing_pages)+int(missing_themes)+int(missing_isbn)\n",
    "\n",
    "print(\"\\nRapport de Webscraping : sur\",len(df_data_webscrap_a_nettoyer), \" livres:\\n\" )\n",
    "print(\"\\t\\tTitres indisponibles =\", missing_livres)\n",
    "print(\"\\t\\tNotes indisponibles =\", missing_notes)\n",
    "print(\"\\t\\tAuteurs indisponibles =\", missing_auteurs)\n",
    "print(\"\\t\\tDates_de_publication indisponibles =\", missing_dates)\n",
    "print(\"\\t\\tEditeurs indisponibles =\", missing_editeurs)\n",
    "print(\"\\t\\tNombres de pages indisponibles =\", missing_pages)\n",
    "print(\"\\t\\tThèmes indisponibles =\", missing_themes)\n",
    "print(\"\\t\\tLivres sans aucun ISBN =\", missing_isbn)\n",
    "print(\"\\t\\tLivres sans image =\", missing_images)\n",
    "\n",
    "objectif = len(df_data_webscrap_a_nettoyer)*9\n",
    "reussite = objectif - total_missing\n",
    "success_rate = reussite / objectif*100\n",
    "print(\"\\n\\t Taux de réussite des extractions =\", success_rate,\"%\")\n",
    "if success_rate >= 92:\n",
    "    print(\"\\n\\t Excellent travail, Johnson.\\n\")\n",
    "elif success_rate >= 86:\n",
    "    print(\"\\n\\t Pas mal, Johnson.\\n\")\n",
    "elif success_rate >= 78:\n",
    "    print(\"\\n\\t C'est presque mauvais, Johnson.\\n\")\n",
    "else :\n",
    "    print(\"\\n\\t Je ne vous paye pas pour des résultats aussi mauvais, Johnson.\\n\")\n",
    "\n",
    "\n",
    "#csv pour nettoyage automatisé\n",
    "try:\n",
    "    df_data_webscrap_a_nettoyer.to_csv('df_data_webscrap_a_nettoyer.csv', index=False)\n",
    "    print('Le fichier \"df_data_webscrap_a_nettoyer.csv\" a bien été généré.')\n",
    "except Exception as e:\n",
    "    print(f\"Erreur lors de la création du fichier CSV : {e}\")\n",
    "\n",
    "\n",
    "#csv pour archive\n",
    "try:\n",
    "    a = {datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}\n",
    "    a = re.sub(\"['{}]\",\"\",re.sub(\"[-: ]\",\"_\",str(a)))\n",
    "    df_data_webscrap_a_nettoyer.to_csv(f'archives_csv/df_data_webscrap_a_nettoyer_{a}.csv', index=False)\n",
    "    print('Le fichier csv d\\'archive a bien été généré.')\n",
    "except Exception as e:\n",
    "    print(f\"Erreur lors de la création du fichier d\\'archive : {e}\")\n",
    "\n",
    "end_time = time.time()  \n",
    "execution_time = (end_time - start_time)/60\n",
    "print(f\"Temps d'exécution : {execution_time:.4f} minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008dc7dc-d391-45f2-9993-34ce434d6547",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "<h1>Script d'insertion en BDD MySql - Mongo </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a907f0-3893-4717-b12b-fa48ba1fea91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREER LA BASE DE DONNEES DANS L'INVITE DE COMMANDE :\n",
    "# Se mettre dans le dossier qui contient fichier.sql puis écrire la commande :\n",
    "# \"C:\\Program Files\\MySQL\\MySQL Server 8.0\\bin\\mysql.exe\" < bdd.sql -u root -p\n",
    "from datetime import datetime\n",
    "import mysql.connector\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pymongo\n",
    "from pymongo import MongoClient\n",
    "print(\"\\nExécution du script d'insertion:\",datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "load_dotenv()\n",
    "\n",
    "################################################################### Partie MySQL\n",
    "user = os.getenv('USER')\n",
    "password = os.getenv('PASSWORD')\n",
    "host = os.getenv('HOST')\n",
    "database = os.getenv('DATABASE')\n",
    "\n",
    "connexion = mysql.connector.connect(\n",
    "    host=host,\n",
    "    user=user,\n",
    "    password=password,\n",
    "    database=database\n",
    ")\n",
    "curseur = connexion.cursor()\n",
    "# Le curseur est utilisé pour exécuter des requêtes SQL\n",
    "# curseur.execute(\"select * from livres;\")\n",
    "# for i in curseur:\n",
    "#      print(i[0])\n",
    "\n",
    "    \n",
    "def remplir_sql(table : str, csv : str, connexion):\n",
    "    \"\"\"Fonction qui prend en argument la table à remplir en str, le csv source en str, et la connexion mysql.connector.\n",
    "    Le passage en csv fait perdre les d-types des colonnes pandas. Ici, ils seront retypés avec astype() pour permettre une transformation des \n",
    "    NaN en None afins que ces valeurs manquantes soient admises par MySQL\"\"\"\n",
    "    df = pd.read_csv(csv)\n",
    "    if csv == \"table_livres.csv\":\n",
    "        df[\"ISBN_defaut\"] = df[\"ISBN_defaut\"].astype(\"str\") #df[\"ISBN_defaut\"] ressort des floats. Il faut traiter ça\n",
    "        def modify_isbn(value):\n",
    "            try:\n",
    "                return value[:-2] if float(value) > 5 else value\n",
    "            except ValueError:\n",
    "                return value\n",
    "        df[\"ISBN_defaut\"] = df[\"ISBN_defaut\"].apply(modify_isbn)\n",
    "        df[\"ISBN_defaut\"] = df[\"ISBN_defaut\"].replace(\"nan\", None)\n",
    "        df[\"ISBN_10\"] = df[\"ISBN_10\"].astype(\"str\")\n",
    "        df[\"ISBN_10\"] = df[\"ISBN_10\"].apply(modify_isbn)\n",
    "        df[\"ISBN_10\"] = df[\"ISBN_10\"].replace(\"nan\", None)\n",
    "        df[\"livre_nombre_de_pages\"] = df[\"livre_nombre_de_pages\"].astype(\"object\")\n",
    "        df[\"livre_note\"] = df[\"livre_note\"].astype(\"object\")\n",
    "        df[\"ISBN_verifie\"] = df[\"ISBN_verifie\"].astype(bool)\n",
    "        df[\"livre_date\"] = df[\"livre_date\"].astype(\"object\")\n",
    "        df = df.where(pd.notna(df), None) # where(condition, autre_valeur) conserve les valeurs condition == True sinon remplace par autre_valeur\n",
    "    else:\n",
    "        df = df.where(pd.notna(df), None) \n",
    "        \n",
    "    # Récupération des colonnes du dataframe au format string pour définir les colonnes cibles dans la requête sql\n",
    "    colonnes = ', '.join(df.columns)\n",
    "    # On génère le bon nombre de placeholders (les %s) au format string pour définir les valeurs à insérer dans la requête sql\n",
    "    placeholders = ', '.join(['%s'] * len(df.columns))\n",
    "    sql = f\"INSERT INTO {table} ({colonnes}) VALUES ({placeholders})\"\n",
    "    # Définition des valeurs avec des tuples :\n",
    "    # df.itertuples() est une méthode qui parcourt un DataFrame et extrait les lignes sous forme de tuples, \n",
    "    # ce qui est parfait pour executemany()\n",
    "    valeurs = [tuple(row) for row in df.itertuples(index=False, name=None)]\n",
    "    curseur = connexion.cursor()\n",
    "    curseur.executemany(sql, valeurs)\n",
    "    connexion.commit()\n",
    "    curseur.close()\n",
    "    print(\"Inserts réussis dans\",table)\n",
    "\n",
    "remplir_sql(\"Livres\", \"table_livres.csv\", connexion)\n",
    "remplir_sql(\"Auteurs\", \"table_auteurs.csv\", connexion)\n",
    "remplir_sql(\"Editeurs\", \"table_editeurs.csv\", connexion)\n",
    "remplir_sql(\"Thematiques\", \"table_thematiques.csv\", connexion)\n",
    "remplir_sql(\"auteurs_livres\", \"table_auteurs_livres.csv\", connexion)\n",
    "remplir_sql(\"editeurs_livres\", \"table_editeurs_livres.csv\", connexion)\n",
    "remplir_sql(\"thematiques_livres\", \"table_thematiques_livres.csv\", connexion)\n",
    "\n",
    "connexion.close()\n",
    "\n",
    "################################################################### Partie MongoDB\n",
    "host = os.getenv('MONGOHOST')\n",
    "port = int(os.getenv('MONGOPORT'))\n",
    "database = os.getenv('MONGODATABASE')\n",
    "collection = os.getenv('MONGOCOLLECTION')\n",
    "\n",
    "client = MongoClient(host, port)\n",
    "db = client[database]\n",
    "images = db[collection]\n",
    "\n",
    "def remplir_mongo(csv : str):\n",
    "    \"\"\"Remplit la BDD locale MongoDB avec les images du fichier CSV issu du webscraping.\"\"\"\n",
    "    df = pd.read_csv(csv)\n",
    "    for i in range(len(df)):\n",
    "        dico = {\n",
    "            f\"image_id\" : int(df['image_id'].loc[i]),\n",
    "            f\"image\" : str(df['image'].loc[i]),\n",
    "            }\n",
    "        x = images.insert_one(dico)\n",
    "    print(len(df),\"Images insérées dans MongoDB\")\n",
    "\n",
    "remplir_mongo(\"collection_images.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e3ab22-39e2-4ed9-9317-a08a9743429d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "<h1>Lire les images en base 64</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b66fddd-2a21-40f0-8820-d9b0a238b879",
   "metadata": {},
   "source": [
    "<h2>Depuis le notebook</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d275dfdf-eecb-4271-b4c6-eb0c9c5d95fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "\n",
    "image_base64 = df_images.loc[0, 'images']\n",
    "image_bytes = base64.b64decode(image_base64)\n",
    "image = Image.open(BytesIO(image_bytes))\n",
    "image.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f1454d-df6e-42cb-9cde-bf39dd8febf0",
   "metadata": {},
   "source": [
    "<h2>Depuis MongoDB</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b18fcb79-1602-4827-8236-74524bed8611",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import base64\n",
    "from pymongo import MongoClient\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "\n",
    "host = os.getenv('MONGOHOST')\n",
    "port = int(os.getenv('MONGOPORT'))\n",
    "database = os.getenv('MONGODATABASE')\n",
    "collection = os.getenv('MONGOCOLLECTION')\n",
    "\n",
    "client = MongoClient(host, port)\n",
    "db = client[database]\n",
    "images = db[collection]\n",
    "\n",
    "\n",
    "document = images.find_one({\"image_id\": 44})\n",
    "\n",
    "if document:\n",
    "    image_base64 = document[\"image\"]  \n",
    "    image_data = base64.b64decode(image_base64)\n",
    "    image = Image.open(BytesIO(image_data))\n",
    "    image.show()\n",
    "else:\n",
    "    print(\"Image non trouvée !\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae05d225-c9fa-4165-8f08-7036521bffa8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "<h1>Script de nettoyage et d'agrégation des données</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a91c9495-8147-4e0d-95f6-07ec82959181",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Début de l'exécution du script de nettoyage : 2025-03-02 13:21:03\n",
      "Tous les livres du dernier webscraping ( 181 ) sont déjà connus dans la base de données. Aucun csv ne sera généré.\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Utilisateur\\anaconda3\\envs\\projet_bloc_1\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3585: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "# Ce script gère le nettoyage et l'agrégation des données, y compris pour la première itération.\n",
    "\n",
    "import mysql.connector\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from datetime import datetime\n",
    "import csv\n",
    "import pandas as pd\n",
    "from unidecode import unidecode\n",
    "import re\n",
    "from deep_translator import GoogleTranslator\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "\n",
    "print(\"\\nDébut de l'exécution du script de nettoyage :\",datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "#####################################################################################################################################\n",
    "##### MYSQL : RECUPERER LES DERNIERS ID ET LES VARIABLES DEJA PRESSENTES DANS LA BDD POUR GERER LES INSERTIONS ET LES DOUBLONS ######\n",
    "#####################################################################################################################################\n",
    "#####################################################################################################################################\n",
    "\n",
    "# user = os.getenv('USER') Je n'arrive pas pour l'instant à gérer un conflit entre user = pierre pour WSL et user = root(ce qui est attendu)\n",
    "user = 'root'\n",
    "# pour les autres variables cachées ça marche, WSL va bien les chercher\n",
    "password = os.getenv('PASSWORD')\n",
    "host = os.getenv('HOST')\n",
    "database = os.getenv('DATABASE')\n",
    "\n",
    "connexion = mysql.connector.connect(\n",
    "    host=host,\n",
    "    user=user,\n",
    "    password=password,\n",
    "    database=database\n",
    ")\n",
    "curseur = connexion.cursor()\n",
    "\n",
    "query_1 = \"SELECT MAX(livre_id) FROM livres;\"\n",
    "curseur.execute(query_1)\n",
    "result = curseur.fetchone() \n",
    "current_livre_id = (result[0] or 0)  # Pour python 5 or 0 = 5. Ca permet de gérer le None de primo nettoyage\n",
    "\n",
    "query_2 = \"SELECT MAX(auteur_id) FROM auteurs;\"\n",
    "curseur.execute(query_2)\n",
    "result = curseur.fetchone() \n",
    "current_auteur_id = (result[0] or 0)\n",
    "\n",
    "query_3 = \"SELECT MAX(editeur_id) FROM editeurs;\"\n",
    "curseur.execute(query_3)\n",
    "result = curseur.fetchone() \n",
    "current_editeur_id = (result[0] or 0)\n",
    "\n",
    "query_4 = \"SELECT MAX(thematique_id) FROM thematiques;\"\n",
    "curseur.execute(query_4)\n",
    "result = curseur.fetchone() \n",
    "current_thematique_id = (result[0] or 0)\n",
    "\n",
    "livres_deja_dans_la_bdd  =[]\n",
    "query_a = \"select livre_nom from livres;\"\n",
    "curseur.execute(query_a)\n",
    "for i in curseur:\n",
    "    livres_deja_dans_la_bdd.append(i[0])\n",
    "\n",
    "auteurs_deja_dans_la_bdd  =[]\n",
    "query_b = \"select auteur_nom from auteurs;\"\n",
    "curseur.execute(query_b)\n",
    "for i in curseur:\n",
    "    if i[0] != None:\n",
    "        auteurs_deja_dans_la_bdd.append(i[0])\n",
    "\n",
    "editeurs_deja_dans_la_bdd  =[]\n",
    "query_c = \"select editeur_nom from editeurs;\"\n",
    "curseur.execute(query_c)\n",
    "for i in curseur:\n",
    "    if i[0] != None:\n",
    "        editeurs_deja_dans_la_bdd.append(i[0])\n",
    "\n",
    "thematiques_deja_dans_la_bdd  =[]\n",
    "query_d = \"select thematique_nom from thematiques;\"\n",
    "curseur.execute(query_d)\n",
    "for i in curseur:\n",
    "    if i[0] != None:\n",
    "        thematiques_deja_dans_la_bdd.append(i[0])\n",
    "\n",
    "dico_auteurs_deja_presents = []\n",
    "query_00 = \"select * from auteurs;\"\n",
    "curseur.execute(query_00)\n",
    "for i in curseur:\n",
    "    dico_auteurs_deja_presents.append(i)\n",
    "dico_auteurs_deja_presents = dict(dico_auteurs_deja_presents)\n",
    "\n",
    "dico_editeurs_deja_presents = []\n",
    "query_01 = \"select * from editeurs;\"\n",
    "curseur.execute(query_01)\n",
    "for i in curseur:\n",
    "    dico_editeurs_deja_presents.append(i)\n",
    "dico_editeurs_deja_presents = dict(dico_editeurs_deja_presents)\n",
    "\n",
    "dico_themes_deja_presents = []\n",
    "query_02 = \"select * from thematiques;\"\n",
    "curseur.execute(query_02)\n",
    "for i in curseur:\n",
    "    dico_themes_deja_presents.append(i)\n",
    "dico_themes_deja_presents = dict(dico_themes_deja_presents)\n",
    "\n",
    "curseur.close()\n",
    "connexion.close()\n",
    "\n",
    "#####################################################################################################################################\n",
    "##################################################### DEBUT DU NETTOYAGE ############################################################\n",
    "#####################################################################################################################################\n",
    "#####################################################################################################################################\n",
    "\n",
    "df = pd.read_csv(\"archives_csv/df_data_webscrap_a_nettoyer_2025-02-19_14_09_11.csv\")\n",
    "\n",
    "# Premier dataframe. Il enverra ses colonnes dans des dataframes secondaires, qui auront vocation à devenir spécifiquement chacun des\n",
    "# 8 CSV d'insertion pour les 7 tables de la BDD MySQL + 1 collection d'images pour la BDD MongoDB.\n",
    "\n",
    "df[\"livre_nom\"] = df[\"livre_nom\"].apply(\n",
    "    lambda x : unidecode(x)\n",
    "    )\n",
    "df[\"livre_nom\"] = df[\"livre_nom\"].apply(\n",
    "    lambda x : x[:255]\n",
    "    )\n",
    "df[\"livre_nom\"] = df[\"livre_nom\"].apply(\n",
    "    lambda x : x.lower()\n",
    "    )\n",
    "df = df.drop_duplicates(subset=['livre_nom'])\n",
    "df = df.drop(df[df['livre_nom'].isin(['mein kampf','my struggle'])].index)\n",
    "df = df.reset_index(drop = True)\n",
    "\n",
    "# C'est ici que l'ID est généré, après le drop duplicate par titre. Il reprend à n+1 avec n = current_livre_id\n",
    "df[\"id\"] = df.index + current_livre_id + 1\n",
    "\n",
    "df[\"livre_note\"] = df[\"livre_note\"].apply(\n",
    "    lambda x : re.sub(r\"\\(.*?\\)\", \"\", x)\n",
    "    )\n",
    "\n",
    "df[\"livre_date\"] = df[\"livre_date\"].apply(\n",
    "    lambda x : re.findall(r'\\d{4}', x)\n",
    "    )\n",
    "\n",
    "df[\"livre_date\"] = df[\"livre_date\"].apply(lambda x: x[0] if x else 0)\n",
    "df[\"livre_date\"] = df[\"livre_date\"].fillna(0).astype(int)\n",
    "df.loc[~df[\"livre_date\"].between(1901, 2100), \"livre_date\"] = None #le format YEAR SQL considère valide la plage entre 1901 et 20155\n",
    "df[\"livre_date\"] = pd.to_datetime(df[\"livre_date\"], format=\"%Y\").dt.year\n",
    "df[\"livre_date\"] = df[\"livre_date\"].replace({pd.NA: None, float(\"nan\"): None})\n",
    "\n",
    "df[\"livre_id\"] = df[\"id\"]\n",
    "\n",
    "df[\"auteur_nom\"] = df[\"auteur_nom\"].apply(\n",
    "    lambda x : unidecode(x).strip()\n",
    "    )\n",
    "\n",
    "df[\"editeur_nom\"] = df[\"editeur_nom\"].apply(\n",
    "    lambda x : unidecode(x)\n",
    "    )\n",
    "\n",
    "df[\"thematique_nom\"] = df[\"thematique_nom\"].apply(\n",
    "    lambda x : re.sub(r\"[\\[\\]]\", \"\", x)  \n",
    ")\n",
    "df[\"thematique_nom\"] = df[\"thematique_nom\"].apply(\n",
    "    lambda x : x.split(\",\")\n",
    ")\n",
    "pd.set_option('future.no_silent_downcasting', True)\n",
    "df.replace(\"indisponible\", None, inplace=True)\n",
    "df.replace('ISBN pas encore vérifié', 0, inplace=True)\n",
    "\n",
    "nombre_de_livres_au_début = len(df)\n",
    "# gestion des doublons par titre avec un masque booleen\n",
    "masque = df[\"livre_nom\"].isin(livres_deja_dans_la_bdd)\n",
    "# Supprimer les lignes où le masque est True\n",
    "df = df[~masque]\n",
    "# A partir de là je sais que tous les \"livre_id\" resteront jusqu'à la fin du script.\n",
    "# Je conclus donc que les tables de jonction comporteront, premières ou pas, tous ces \"livre_id\".\n",
    "# Ce n'est pas le cas des \n",
    "# auteurs/editeurs/thematiques\n",
    "# et\n",
    "# auteurs/editeurs/thematiques_id\n",
    "# qui pour certains seront des doublons de ce que j'avais déjà dans la BDD : ils seront droppés avant la fin du script.\n",
    "\n",
    "if not df.empty:\n",
    "    print(\"Le dernier webscraping apporte\",len(df),\"nouveaux livres. (sur les\",nombre_de_livres_au_début,\"qui ont été extraits.\")\n",
    "else:\n",
    "    print(\"Tous les livres du dernier webscraping (\",nombre_de_livres_au_début,\") sont déjà connus dans la base de données. Aucun csv ne sera généré.\")\n",
    "    sys.exit()\n",
    "\n",
    "#####################################################################################################################################\n",
    "################################################ AGREGATION : UN DATAFRAME PAR TABLE ################################################\n",
    "#####################################################################################################################################\n",
    "#####################################################################################################################################\n",
    "\n",
    "###########################################\n",
    "################ df_livres ################\n",
    "###########################################\n",
    "df_livres = pd.DataFrame()\n",
    "df_livres[\"livre_id\"] = df[\"livre_id\"]\n",
    "df_livres[\"livre_nom\"] = df[\"livre_nom\"]\n",
    "df_livres[\"livre_nombre_de_pages\"] = df[\"livre_nombre_de_pages\"]\n",
    "df_livres[\"livre_note\"] = df[\"livre_note\"]\n",
    "df_livres[\"ISBN_10\"] = df[\"ISBN_10\"]\n",
    "df_livres[\"ISBN_13\"] = df[\"ISBN_13\"]\n",
    "df_livres[\"ISBN_defaut\"] = df[\"ISBN_defaut\"]\n",
    "df_livres[\"ISBN_verifie\"] = df[\"ISBN_verifie\"]\n",
    "df_livres[\"livre_date\"] = df[\"livre_date\"]\n",
    "\n",
    "###########################################\n",
    "##### df_auteurs et df_auteurs_livres #####\n",
    "###########################################\n",
    "def assigne_un_auteur_id(auteur):\n",
    "    \"\"\"Pour un auteur, assigne l'id en interrogeant le dictionnaire auteurs:id de la base des données ou en génère un nouveau si besoin\"\"\"\n",
    "    global current_auteur_id  # Utilisation de la variable globale current_id = je vais la mettre à jour en dehors du scope de la fonction\n",
    "    for auteur_id, auteur_nom in dico_auteurs_deja_presents.items(): # .items() déstructure le dictionnaire en donnant une vue sur des tuples\n",
    "        if auteur == None:\n",
    "            return 1\n",
    "        if auteur_nom == auteur:\n",
    "            return auteur_id\n",
    "    current_auteur_id += 1   \n",
    "    dico_auteurs_deja_presents[current_auteur_id] = auteur\n",
    "    return current_auteur_id\n",
    "\n",
    "df[\"auteur_id\"] = df[\"auteur_nom\"].apply(assigne_un_auteur_id)\n",
    "\n",
    "df_auteurs = pd.DataFrame()\n",
    "df_auteurs[\"auteur_nom\"] = df[\"auteur_nom\"]\n",
    "df_auteurs[\"auteur_id\"] = df[\"auteur_id\"]\n",
    "\n",
    "df_auteurs = df_auteurs.drop_duplicates(subset=[\"auteur_nom\"])\n",
    "dico_auteurs = df_auteurs.set_index(\"auteur_nom\")[\"auteur_id\"].to_dict()\n",
    "\n",
    "df_auteurs_livres = pd.DataFrame() # c'est ma table de jonction\n",
    "\n",
    "df_auteurs_livres[\"livre_id\"] = df[\"livre_id\"]\n",
    "df_auteurs_livres[\"auteur_id\"] = df[\"auteur_nom\"] # j'ai pu me permettre de faire un drop duplicates sur df_auteurs car j'avais conservé la ligne dans ce df originel\n",
    "df_auteurs_livres[\"auteur_id\"] = df_auteurs_livres[\"auteur_id\"].map(dico_auteurs)\n",
    "# map remplace chaque valeur de \"auteur_id\" par son ID en cherchant dans dico_auteurs\n",
    "# ! Si aucun nom ne correspond dans dico_auteurs, la valeur sera remplacée par NaN \n",
    "df_auteurs_livres = df_auteurs_livres.dropna()\n",
    "\n",
    "# je peux maintenant finir df_auteurs en supprimant ce qui sera un doublon lors de l'insertion SQL\n",
    "\n",
    "masque_auteurs = df_auteurs[\"auteur_nom\"].isin(auteurs_deja_dans_la_bdd)\n",
    "df_auteurs = df_auteurs[~masque_auteurs] # supprimer les lignes où le masque est True \n",
    "df_auteurs = df_auteurs.dropna(subset=[\"auteur_nom\"])\n",
    "\n",
    "#############################################\n",
    "##### df_editeurs et df_editeurs_livres #####\n",
    "#############################################\n",
    "def assigne_un_editeur_id(editeur):\n",
    "    \"\"\"Pour un editeur, assigne l'id en interrogeant le dictionnaire editeurs:id de la base des données ou en génère un nouveau si besoin\"\"\"\n",
    "    global current_editeur_id  \n",
    "    for editeur_id, editeur_nom in dico_editeurs_deja_presents.items(): \n",
    "        if editeur == None:\n",
    "            return 1\n",
    "        if editeur_nom == editeur:\n",
    "            return editeur_id\n",
    "    current_editeur_id += 1 \n",
    "    dico_editeurs_deja_presents[current_editeur_id] = editeur\n",
    "    return current_editeur_id\n",
    "\n",
    "df_editeurs = pd.DataFrame()\n",
    "df_editeurs[\"editeur_id\"] = df[\"editeur_nom\"].apply(assigne_un_editeur_id)\n",
    "df_editeurs[\"editeur_nom\"] = df[\"editeur_nom\"]\n",
    "df_editeurs[\"conservation_temporaire_de_livre_id\"]= df[\"livre_id\"]\n",
    "\n",
    "df_editeurs_livres = pd.DataFrame()\n",
    "df_editeurs_livres[\"editeur_id\"] = df_editeurs[\"editeur_id\"]\n",
    "df_editeurs_livres[\"livre_id\"] = df_editeurs[\"conservation_temporaire_de_livre_id\"]\n",
    "df_editeurs_livres = df_editeurs_livres.dropna()\n",
    "df_editeurs_livres.drop_duplicates()\n",
    "\n",
    "masque_editeurs = df_editeurs[\"editeur_nom\"].isin(editeurs_deja_dans_la_bdd)\n",
    "\n",
    "df_editeurs = df_editeurs[~masque_editeurs]\n",
    "df_editeurs = df_editeurs.drop(columns=\"conservation_temporaire_de_livre_id\")\n",
    "df_editeurs = df_editeurs.drop_duplicates(subset=[\"editeur_nom\"])\n",
    "df_editeurs = df_editeurs.dropna()\n",
    "\n",
    "###################################################\n",
    "##### df_thematiques et df_thematiques_livres #####\n",
    "###################################################\n",
    "def assigne_un_thematique_id(thematique):\n",
    "    \"\"\"Pour une thematique, assigne l'id en interrogeant le dictionnaire thematiques:id de la base des données ou en génère un nouveau si besoin\"\"\"\n",
    "    global current_thematique_id  \n",
    "    for thematique_id, thematique_nom in dico_themes_deja_presents.items(): \n",
    "        if thematique == None:\n",
    "            return 1\n",
    "        if thematique_nom == thematique:\n",
    "            return thematique_id\n",
    "    current_thematique_id += 1 \n",
    "    dico_themes_deja_presents[current_thematique_id] = thematique\n",
    "    return current_thematique_id\n",
    "\n",
    "def translate_tag_to_french(tag):\n",
    "    \"\"\"Traduit un tag/thématique. Détecte la langue d'origine et retourne la traduction en français.\"\"\"\n",
    "    traduction = GoogleTranslator(source='auto', target='fr').translate(tag)\n",
    "    return traduction\n",
    "\n",
    "\n",
    "future_colonne = []\n",
    "for i in range(len(df)):\n",
    "    ligne = []\n",
    "    x = int(df[\"livre_id\"].iloc[i])\n",
    "    for j in df[\"thematique_nom\"].iloc[i][:10]:\n",
    "        tuples = (x,j)\n",
    "        ligne.append(tuples)\n",
    "    future_colonne.append(ligne)\n",
    "\n",
    "df[\"thematique_tuples\"] = future_colonne\n",
    "tqdm.pandas(desc=\"Traduction des tags\")\n",
    "def traite_la_liste_de_tags(x):\n",
    "    liste=[]\n",
    "    for i in x:\n",
    "        tag = i[1]\n",
    "        tag = unidecode(tag.lower())\n",
    "        tag = translate_tag_to_french(tag)\n",
    "        tag = re.sub(\"[éèêë]\",\"e\",tag)\n",
    "        tag = re.sub(\"[âà]\",\"a\",tag)\n",
    "        tag = re.sub(\"[ûü]\",\"u\",tag)\n",
    "        tag = re.sub(\"[ôö]\",\"o\",tag)\n",
    "        tag = re.sub(\"[^a-zA-Z çïœŒ']\",\"\",tag)\n",
    "        tag = re.sub(\"^'|'$\", \"\", tag)\n",
    "        tag = re.sub(\"^'|'$\", \"\", tag)\n",
    "        tag = tag.lower()\n",
    "        tag = tag.lstrip()\n",
    "        tag = tag.rstrip()\n",
    "        if \"abus\" in tag:\n",
    "            tag = \"criminalite\"\n",
    "        tag = i[0],tag\n",
    "        liste.append(tag)\n",
    "    liste = list(set(liste))\n",
    "    return liste\n",
    "\n",
    "df[\"thematique_tuples\"] = df[\"thematique_tuples\"].progress_apply(traite_la_liste_de_tags)\n",
    "\n",
    "df_thematiques = pd.DataFrame()\n",
    "thematiques_du_dataset = []\n",
    "conservation_des_livre_id = []\n",
    "for i in range(len(df)):\n",
    "    x = df[\"thematique_tuples\"].iloc[i]\n",
    "    for y in x:\n",
    "        thematiques_du_dataset.append(y[1])\n",
    "        conservation_des_livre_id.append(y[0])\n",
    "\n",
    "df_thematiques[\"thematique_nom\"] = pd.DataFrame(thematiques_du_dataset)\n",
    "df_thematiques[\"conservation_temporaire_de_livre_id\"] = pd.DataFrame(conservation_des_livre_id)\n",
    "df_thematiques = df_thematiques.sort_values(by=\"thematique_nom\")\n",
    "df_thematiques = df_thematiques.reset_index(drop = True)\n",
    "df_thematiques[\"thematique_id\"] = df_thematiques[\"thematique_nom\"].apply(assigne_un_thematique_id)\n",
    "dico_thematiques = df_thematiques.set_index(\"thematique_nom\")[\"thematique_id\"].to_dict()\n",
    "\n",
    "df_thematiques_livres = pd.DataFrame()\n",
    "df_thematiques_livres[\"livre_id\"] = df_thematiques[\"conservation_temporaire_de_livre_id\"]\n",
    "df_thematiques_livres[\"thematique_id\"] = df_thematiques[\"thematique_id\"]\n",
    "df_thematiques_livres = df_thematiques_livres.dropna()\n",
    "df_thematiques_livres = df_thematiques_livres.drop_duplicates()\n",
    "\n",
    "df_thematiques = df_thematiques.drop_duplicates(subset=\"thematique_nom\")\n",
    "df_thematiques = df_thematiques.drop(columns=\"conservation_temporaire_de_livre_id\")\n",
    "\n",
    "masque_thematiques = df_thematiques[\"thematique_nom\"].isin(thematiques_deja_dans_la_bdd)\n",
    "df_thematiques = df_thematiques[~masque_thematiques] # supprimer les lignes où le masque est True \n",
    "df_thematiques = df_thematiques.dropna(subset=[\"thematique_nom\"]) # préventif\n",
    "\n",
    "#################################\n",
    "########### df_images ###########\n",
    "#################################\n",
    "df_images = pd.DataFrame()\n",
    "df_images[\"image\"] = df[\"images_openlibrary\"]\n",
    "df_images[\"image_id\"] = df[\"id\"]\n",
    "df_images = df_images.fillna(\"indisponible\")\n",
    "\n",
    "#####################################################################################################################################\n",
    "############################################### GENERATION DES CSV POUR CHAQUE TABLE ################################################\n",
    "#####################################################################################################################################\n",
    "#####################################################################################################################################\n",
    "\n",
    "#csv pour insertion automatisée\n",
    "df_livres.to_csv('table_livres.csv', index=False)\n",
    "df_auteurs.to_csv('table_auteurs.csv', index=False)\n",
    "df_editeurs.to_csv('table_editeurs.csv', index=False)\n",
    "df_thematiques.to_csv('table_thematiques.csv', index=False)\n",
    "df_auteurs_livres.to_csv('table_auteurs_livres.csv', index=False)\n",
    "df_thematiques_livres.to_csv('table_thematiques_livres.csv', index=False)\n",
    "df_editeurs_livres.to_csv('table_editeurs_livres.csv', index=False)\n",
    "df_images.to_csv('collection_images.csv', index=False)\n",
    "\n",
    "#csv d'insertion pour archive\n",
    "a = {datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}\n",
    "a = re.sub(\"['{}]\",\"\",re.sub(\"[-: ]\",\"_\",str(a)))\n",
    "\n",
    "df_livres.to_csv(f'archives_csv/table_livres{a}.csv', index=False)\n",
    "df_auteurs.to_csv(f'archives_csv/table_auteurs{a}.csv', index=False)\n",
    "df_editeurs.to_csv(f'archives_csv/table_editeurs{a}.csv', index=False)\n",
    "df_thematiques.to_csv(f'archives_csv/table_thematiques{a}.csv', index=False)\n",
    "df_auteurs_livres.to_csv(f'archives_csv/table_auteurs_livres{a}.csv', index=False)\n",
    "df_thematiques_livres.to_csv(f'archives_csv/table_thematiques_livres{a}.csv', index=False)\n",
    "df_editeurs_livres.to_csv(f'archives_csv/table_editeurs_livres{a}.csv', index=False)\n",
    "df_images.to_csv(f'archives_csv/collection_images{a}.csv', index=False)\n",
    "\n",
    "print(\"Les fichier csv pour les insertions dans les tables ont bien été générés.\")\n",
    "print(\"\\nFin de l'exécution du script de nettoyage :\",datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377944ba-20d0-4364-97b8-53007a0ad08b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "<h1>Tests pour l'intégrité référentielle de la BDD MySQL</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae709fc4-49a3-49d2-a2aa-467e757882a7",
   "metadata": {},
   "source": [
    "J'ai dupliqué ma base de données dans sqlworkbench avec server data export/import (fichier dump)<br>\n",
    "Cela, afin de faire des tests avec DELETE sans risquer des suppressions accidentelles en casade dans ma base de données principale."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7360a1-5d3c-49ad-a03a-ed5991c0c2dd",
   "metadata": {},
   "source": [
    "Dans database = \"test_db\"<br><br>\n",
    "Avant les DELETE, j'ai :<br><br>\n",
    "SELECT * FROM livres WHERE livre_id = 1500;<br>\n",
    "('288.0', 'strategic brand management', None, 1500, None, '9782857901198', 2007, 0, '9780199260003')\n",
    "\n",
    "SELECT * FROM auteurs_livres WHERE livre_id = 1500;<br>\n",
    "(1500, 1139)\n",
    "\n",
    "SELECT * FROM auteurs WHERE auteur_id = 1139;<br>\n",
    "(1139, 'Richard Elliott')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ed96ccc4-5d40-4d72-a3ec-50db61aa625d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connexion à la base de données réussie !\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import mysql.connector\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "user = os.getenv('USER')\n",
    "password = os.getenv('PASSWORD')\n",
    "host = os.getenv('HOST')\n",
    "database = \"test_db\"\n",
    "try:\n",
    "    mydb = mysql.connector.connect(\n",
    "        user=user,\n",
    "        password=password,\n",
    "        host=host,\n",
    "        database=database\n",
    "    )\n",
    "    cursor = mydb.cursor()\n",
    "    print(\"Connexion à la base de données réussie !\")\n",
    "except mysql.connector.Error as err:\n",
    "    print(f\"Erreur de connexion : {err}\")\n",
    "\n",
    "cursor.execute(\"\"\"\n",
    "delete from livres where livre_id = 1500;\n",
    "\"\"\")\n",
    "for i in cursor:\n",
    "     print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3395d75a-c3a1-49a4-bd24-64024e354460",
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute(\"\"\"\n",
    "select * from livres where livre_id = 1500;\n",
    "\"\"\")\n",
    "for i in cursor:\n",
    "     print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4465cee-ecb2-4106-a28a-fdc52444ec76",
   "metadata": {},
   "source": [
    "La suppression du livre a bien fonctionné"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "908b1023-4847-4348-a558-2fffb8297b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute(\"\"\"\n",
    "select * from auteurs_livres where livre_id = 1500;\n",
    "\"\"\")\n",
    "for i in cursor:\n",
    "     print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20970883-5ce1-4fb5-9692-44b6328ff24b",
   "metadata": {},
   "source": [
    "La suppression en cascade sur la table de jonction a bien fonctionné."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1ecdb721-3736-42b5-862f-96833ed08dd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1139, 'Richard Elliott')\n"
     ]
    }
   ],
   "source": [
    "cursor.execute(\"\"\"\n",
    "select * from auteurs where auteur_id = 1139;\n",
    "\"\"\")\n",
    "for i in cursor:\n",
    "     print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06956af2-87e3-4874-ac60-d78c8b7696d6",
   "metadata": {},
   "source": [
    "La suppression s'est bien arrêtée à l'auteur. Il est conservé dans la table au cas où il est lié à un autre livre."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a52a04-90d8-4d02-90b7-d263b1cdc07b",
   "metadata": {},
   "source": [
    "<h1>FAST API : tests</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82212f3c-c6b4-400b-91a0-58d0bcc96283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token obtenu avec succès !\n",
      "Données récupérées : [{'livre_nombre_de_pages': '390.0', 'livre_nom': 'laut bercerita', 'livre_note': 4.1, 'livre_id': 1, 'ISBN_10': None, 'ISBN_13': '9782895123552', 'livre_date': 2017, 'ISBN_verifie': 0, 'ISBN_defaut': '9786024246945'}, {'livre_nombre_de_pages': '384.0', 'livre_nom': 'the secret keeper of jaipur', 'livre_note': 5.0, 'livre_id': 2, 'ISBN_10': '23812252', 'ISBN_13': None, 'livre_date': 2021, 'ISBN_verifie': 0, 'ISBN_defaut': '9780778331858'}, {'livre_nombre_de_pages': '215.0', 'livre_nom': 'the world of premchand', 'livre_note': None, 'livre_id': 3, 'ISBN_10': None, 'ISBN_13': '2021443582', 'livre_date': 1969, 'ISBN_verifie': 0, 'ISBN_defaut': None}, {'livre_nombre_de_pages': '254.0', 'livre_nom': 'mansarovar - 5', 'livre_note': None, 'livre_id': 4, 'ISBN_10': None, 'ISBN_13': None, 'livre_date': 2021, 'ISBN_verifie': 0, 'ISBN_defaut': '9789390852857'}, {'livre_nombre_de_pages': '274.0', 'livre_nom': 'mansarovar - part 6', 'livre_note': None, 'livre_id': 5, 'ISBN_10': None, 'ISBN_13': None, 'livre_date': 2016, 'ISBN_verifie': 0, 'ISBN_defaut': '9781537252261'}]\n"
     ]
    }
   ],
   "source": [
    "# Lancer préalablement l'api depuis le terminal\n",
    "import requests\n",
    "# Obtenir le token\n",
    "BASE_URL = \"http://127.0.0.1:8000\" \n",
    "token_request = {\"password\": \"librairie\"}\n",
    "response = requests.post(f\"{BASE_URL}/token\", json=token_request)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    token = response.json()[\"token\"]\n",
    "    print(\"Token obtenu avec succès !\")\n",
    "else:\n",
    "    print(\"Erreur lors de la récupération du token :\", response.json())\n",
    "    token = None\n",
    "\n",
    "# Faire une requête à l'API avec le token\n",
    "if token:\n",
    "    headers = {\"Authorization\": f\"Bearer {token}\"}\n",
    "    params = {\"limit\": 10}\n",
    "\n",
    "    response = requests.get(f\"{BASE_URL}/livres\", headers=headers, params=params)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        print(\"Données récupérées :\", response.json())\n",
    "    else:\n",
    "        print(\"Erreur lors de la récupération des données :\", response.json())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd39e889-b92b-465c-a646-3cd7824b34a7",
   "metadata": {},
   "source": [
    "<h1>Notes</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a952c56-914b-4c08-a9d9-55f65a943044",
   "metadata": {},
   "source": [
    "J'ai rencontré cette difficulté avec crontab :  \n",
    "Sous WSL, crontab ne se relançait jamais au redémarrage de l'ordinateur.  \n",
    "Il fallait à chaque fois:  \n",
    "-ouvrir le terminal wsl  \n",
    "-lancer la commande sudo service cron start dans le terminal\n",
    "\n",
    "... ou alors ne jamais éteindre l'ordinateur !!!\n",
    "\n",
    "Solution qui permet de rester avec windows 10 et wsl 1   :  \n",
    "Les tâches au démarrage se trouvent dans le fichier .bashrc  \n",
    "Donc avec  \n",
    "nano ~/.bashrc  \n",
    "j'ai rajouté une instruction pour chaque redémarrage du terminal, le fameux :  \n",
    "sudo service cron start  \n",
    "j'ai par ailleurs désactivé la demande de mot de passe en modifiant le fichier dédié \"sudoers\" via visudo qui est l'éditeur recommandé :  \n",
    "sudo visudo  \n",
    "Ca nous ouvre donc l'édition du fichier sudoers à la fin duquel on rajoute:  \n",
    "utilisateur(mets ton prénom) ALL=(ALL) NOPASSWD: /usr/sbin/service cron start\n",
    "\n",
    "Depuis, à l'ouverture du terminal ubuntu, la première chose qui s'écrit est :  \n",
    "Starting periodic command scheduler cron     OK  \n",
    " \n",
    "Il ne reste plus qu'à automatiser l'ouverture d'ubuntu via windows à chaque redémarrage de l'ordinateur  :\n",
    "\n",
    "je mets le wsl.exe de \"C:\\Windows\\System32\"  \n",
    "dans  \n",
    "\"C:\\Users\\Utilisateur\\AppData\\Roaming\\Microsoft\\Windows\\Start Menu\\Programs\\Startup\"\n",
    "\n",
    "Et ça marche !\n",
    "\n",
    "https://www.mediamarkt.ch/fr/content/ordinateurs-bureau/software-apps/windows-10-demarrage-automatique-suppression-de-programmes#Comment%20supprimer%20des%20programmes%20du%20d%C3%A9marrage%20automatique%20de%20Windows%2010"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa3f6a32-625b-4d71-8343-afafca957cf2",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
